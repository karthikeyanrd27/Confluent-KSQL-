from pyspark import SparkConf, SparkContext
from operator import add
import sys
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
import json
from kafka import SimpleProducer, KafkaClient
from kafka import KafkaProducer
from pyspark.streaming.kafka import KafkaUtils, OffsetRange, TopicAndPartition
import abc, six
import avro.schema
from confluent_kafka.avro.cached_schema_registry_client import CachedSchemaRegistryClient
from confluent_kafka.avro.serializer.message_serializer import MessageSerializer
import pandas as pd
import io, random
from avro.io import DatumWriter
from kafka import (
    SimpleClient, KeyedProducer,
    Murmur2Partitioner, RoundRobinPartitioner)
    import json
import requests
import pprint
from cassandra.cluster import Cluster
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *


#producer = KafkaProducer(bootstrap_servers='localhost:9092')

# To send messages synchronously
kafka = KafkaClient('localhost:9092')
producer = SimpleProducer(kafka)
#producer = KafkaProducer(bootstrap_servers='localhost:9092')

kafka = SimpleClient('localhost:9092')
producer = KeyedProducer(kafka)

TOPIC = 'PLANADVERTISE1'
PARTITION = 0
topicAndPartition = TopicAndPartition(TOPIC, PARTITION)
fromOffsets1 = {topicAndPartition:int(PARTITION)}


# Path to user.avsc avro schema
schema_path="/Users/KarthikeyanDurairaj/Desktop/AdvertiserFileschema.txt"
schema = avro.schema.Parse(open(schema_path).read())

topic1 = 'TBL_MS_ADVERTISER_COMPACT'
y = StructType([StructField("PLAN_ID",LongType(), nullable = False), 
                StructField("OP_TYPE",StringType(), nullable = True),
                StructField("ADVERTISER_ID",LongType(), nullable = True),
                StructField("ADVERTISER_NAME",StringType(), nullable = True),
                StructField("ORIGINAL_PLAN_ID",LongType(), nullable = True),
                StructField("PLAN_NAME",StringType(), nullable = True),
                StructField("PORTFOLIO_PLAN_ID",LongType(), nullable = True),
                StructField("PRG_PLAN_ID",StringType(), nullable = True),
                StructField("PROPERTY_ID",LongType(), nullable = True),
                StructField("REVISION_NO",LongType(), nullable = True),
                StructField("STEW_LINK_ID",LongType(), nullable = True),
                ])




def handler(message):
    records = message.collect()
    for record in records:
        value_all=record[1]
        data = [value_all]
        print (type(data))
        print("thisiskarthikeuan")
        value_al = json.dumps(record[1])
        value_all= str.encode(value_al)
        decod_val = (value_all.decode('utf-8'))
        decod_json = json.loads(decod_val)
 
        var_adv_id  =  int(decod_json['ADVERTISER_ID'])
        var_op_type = str(decod_json['OP_TYPE'])
        var_plan_id = int(decod_json['PLAN_ID'])

        print(var_op_type)
         print(var_adv_id)
        print(var_plan_id)
        print("thisisnavaneethan")
      
        if var_plan_id is not None:
           if var_op_type != 'D':
              df=sparkSess.sparkContext.parallelize(data).toDF(schema = y)
              df.printSchema()
              df.show()
              conf = SparkConf()
              conf.setMaster("local[4]")
              conf.setAppName("Spark Cassandra")
              conf.set("spark.cassandra.connection.host","localhost:9042")
              df.write\
                .format("org.apache.spark.sql.cassandra")\
                .mode('append')\
                .options(table="tbl_plan_advertiser", keyspace="key1")\
                .save()       
           else:
              print("thisiselsecondition")
              cluster = Cluster()
              session = cluster.connect('key1')
              var_tab = 'key1.tbl_plan_advertiser'
              var_plan_id = str(var_plan_id)
              var_cmd_01 = "DELETE FROM %s"%var_tab
              var_cmd_02 = " WHERE \"PLAN_ID\" ='%s'"%var_plan_id
              var_cmd_03 = " and \"OP_TYPE\" ='%s'"%var_op_type
              var_cmd = var_cmd_01+var_cmd_02
              print(var_cmd)
              rows = session.execute(var_cmd)
              schema_registry_client = CachedSchemaRegistryClient(url='http://localhost:8081')
serializer = MessageSerializer(schema_registry_client)

sparkSess = SparkSession.builder.appName("My App").getOrCreate()
sc = sparkSess.sparkContext
#sc = SparkContext(appName="PythonStreamingAvro")
ssc = StreamingContext(sc, 10)
kvs = KafkaUtils.createDirectStream(ssc, ['PLANADVERTISE1'], {"metadata.broker.list": 'localhost:9092'},valueDecoder=serializer.decode_message)
#kvs.pprint()
kvs.foreachRDD(handler)

ssc.start()
ssc.awaitTermination()
